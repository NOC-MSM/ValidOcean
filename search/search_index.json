{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the ValidOcean Documentation","text":"<p>ValidOcean is a python library dedicated to leveraging the cloud to accelerate ocean model validation </p> <p>The library aims to facilitate the reproducible validation of ocean general circulation models using observations and ocean (re)analyses stored in Analysis-Ready &amp; Cloud Optimised (ARCO) datasets.</p> <p>ValidOcean utilises xarray to handle N-D labeled arrays, zarr for the reading &amp; writing of ARCO datasets and xesmf to perform regridding.</p> <ul> <li> <p> Get Started in Minutes</p> <p>Install <code>ValidOcean</code> with <code>pip</code> and get up and running in minutes</p> <p> Quick Start</p> </li> <li> <p> Tutorial</p> <p>Learn by doing with the <code>ValidOcean</code> tutorial</p> <p> Tutorials</p> </li> <li> <p> Observations</p> <p>Explore the available ocean observations</p> <p> Observations</p> </li> <li> <p> Customisation</p> <p>Learn how to add new <code>DataLoaders</code> &amp; custom <code>Aggregators</code></p> <p> Customisation</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>ValidOcean is currently in the pre-alpha development phase, so it is recommended to install the library in editable mode as shown below.</p> <p>To get started, clone and install ValidOcean from GitHub:</p> <pre><code>git clone git@github.com:NOC-MSM/ValidOcean.git\n</code></pre> <p>Next, pip install the ValidOcean library in editable mode:</p> <pre><code>cd ValidOcean\npip install -e .\n</code></pre> Helpful Tip... <ul> <li>We strongly recommend installing the ValidOcean library and its dependencies in a new virtual environment.</li> </ul> <p>The simplest way to create a new virtual environment is to use venv:</p> <pre><code>python3.13 -m venv \"env_validocean\"\n</code></pre> <p>Alternatively, using an existing miniconda or miniforge installation:</p> <pre><code>conda create -n env_validocean python=3.13\n</code></pre>"},{"location":"#an-example-validation-workflow","title":"An Example Validation Workflow","text":"<p>Let's consider a typical validation task for an ocean scientist; evaluating the average sea surface temperature field simulated in an ocean model against available observations.</p> <p>To do this, we would likely start by downloading and pre-processing a satellite-derived sea surface temperature product (here, we'll use NOAA OISSTv2). Next, we would calculate equivalent sea surface temperature climatologies using your ocean model data and ocean observations, before regridding them onto a common grid (either the native grid used by the model or the observations). Finally, we would calculate the difference (or error) between the ocean model and observational climatologies and visualise this with a geographical plot.</p> <p>With ValidOcean, each of the pre-processing &amp; regridding stages above are performed for us and we can visualise the model - observations sea surface temperature error in two simple stages...</p> <p>Stage 1:</p> <p>To perform a validation workflow using ValidOcean, we must first prepare our ocean model data &amp; create a <code>ModelValidator</code> object:</p> <pre><code>    import xarray as xr\n    from ValidOcean import ModelValidator\n\n\n    ds = xr.open_dataset(\"/path/to/ocean/model/sst/data.nc\")\n\n    mv = ModelValidator(mdl_data=ds)\n</code></pre> <p>Stage 2:</p> <p>Let's use the <code>.plot_sst_error()</code> method to visualise the ocean model &amp; OISSTv2 sea surface temperature climatologies and their difference. Below we plot the (model - observation) error using 1991-2020 climatologies of sea surface temperature, where the observations are regridded to the native model grid using bilinear interpolation.</p> <p><pre><code>    mv.plot_sst_error(sst_name='tos_con',\n                      obs_name='OISSTv2',\n                      time_bounds='1991-2020',\n                      freq='total',\n                      regrid_to='model',\n                      method='bilinear',\n                      source_plots=True,\n                      stats=True,\n                      )\n</code></pre> </p> <p>We've just performed our first validation workflow using ValidOcean. Subplots (1) &amp; (2) show the climatological (1991-2020) average sea surface temperature for our ocean model and OISSTv2 observations (regridded onto the model grid). Subplot (2) shows the difference / bias between the ocean model and OISSTv2 observations, where positive (negative) values indicate warmer (colder) sea surface temperatues in the ocean model.</p> <p>To learn more about the validation workflows available in ValidOcean and currently available ocean observations, explore our Tutorial and Observations pages.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Summary</p> <p>This page provides guidance for contributors to the ValidOcean package.</p>"},{"location":"contributing/#contributing-to-validocean","title":"Contributing to ValidOcean","text":"<p>Thank you for your interest in contributing to ValidOcean!</p> <p>We welcome contributions from the community to help improve the validation of ocean general circulation models.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<p>To get started with contributing to ValidOcean, please follow the steps below:</p> <ol> <li>Fork the ValidOcean repository on GitHub.</li> <li>Clone your forked repository to your local machine.</li> <li>Create a new branch for your contribution.</li> <li> <p>Make your changes and improvements to the codebase.</p> </li> <li> <p>Follow the NumPy docstring conventions when adding or modifying docstrings.</p> </li> <li> <p>Follow the PEP 8 style guide when writing code.</p> </li> <li> <p>Test your changes thoroughly to ensure they work as expected.</p> </li> <li> <p>Unit tests should be added using pytest for any new features.</p> </li> <li> <p>Commit your changes with clear and descriptive commit messages.</p> </li> <li>Push your changes to your forked repository.</li> <li>Submit a pull request to the main branch of ValidOcean.</li> </ol>"},{"location":"contributing/#code-guidelines","title":"Code Guidelines","text":"<p>When contributing code to ValidOcean, please adhere to the following guidelines:</p> <ul> <li>Follow the coding style and conventions used in the existing codebase.</li> <li>Write clear and concise code with appropriate comments.</li> <li>Ensure your code is well-tested and does not introduce any regressions.</li> <li>Document any new features or changes in the appropriate sections of the documentation.</li> </ul>"},{"location":"contributing/#bug-reports-and-feature-requests","title":"Bug Reports and Feature Requests","text":"<p>If you find any bugs or have ideas for new features, please open an issue on the ValidOcean GitHub repository.</p> <p>Provide as much detail as possible, including steps to reproduce the issue or a clear description of the desired feature.</p>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<p>When participating in the ValidOcean community, please be respectful and considerate towards others. Follow the code of conduct and engage in constructive discussions.</p> <p>We appreciate your contributions and look forward to working with you to grow &amp; improve ValidOcean!</p>"},{"location":"customisation/","title":"Customisation","text":"<p>Summary</p> <p>This page introduces the advanced features of the ValidOcean package, including adding your own ocean observations with custom DataLoaders &amp; calculating new metrics with custom aggregators.</p>"},{"location":"customisation/#custom-dataloaders","title":"Custom DataLoaders","text":"<p>Section Currently Under Development: Come Back Soon!</p> <p>This section will include a detailed guide on how to extend the ValidOcean observations catalog by creating your own DataLoaders.</p>"},{"location":"customisation/#custom-aggregators","title":"Custom Aggregators","text":"<p>Section Currently Under Development: Come Back Soon!</p> <p>This section will include a detailed guide on how to calculate diagnostics using model &amp; observational data by creating your own aggregator functions.</p>"},{"location":"ex1_getting_started/","title":"Getting Started with ValidOcean","text":"In\u00a0[1]: Copied! <pre># -- Import required libraries -- #\nimport xarray as xr\nfrom ValidOcean import ModelValidator\n</pre> # -- Import required libraries -- # import xarray as xr from ValidOcean import ModelValidator <pre>/home/otooth/miniconda3/envs/env_nemo_validation/lib/python3.12/site-packages/esmpy/interface/loadESMF.py:94: VersionWarning: ESMF installation version 8.8.0, ESMPy version 8.8.0b0\n  warnings.warn(\"ESMF installation version {}, ESMPy version {}\".format(\n</pre> In\u00a0[2]: Copied! <pre># Define url to domain store:\nurl_domain = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/domain/\"\nds_domain = xr.open_zarr(url_domain, consolidated=True)\n\n# Define url to sea surface temperature (tos_con) store:\nurl_tos_con = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/T1m/tos_con/\"\nds_tos_con = xr.open_zarr(url_tos_con, consolidated=True)\n\nds_tos_con\n</pre> # Define url to domain store: url_domain = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/domain/\" ds_domain = xr.open_zarr(url_domain, consolidated=True)  # Define url to sea surface temperature (tos_con) store: url_tos_con = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/T1m/tos_con/\" ds_tos_con = xr.open_zarr(url_tos_con, consolidated=True)  ds_tos_con Out[2]: <pre>&lt;xarray.Dataset&gt; Size: 281MB\nDimensions:        (y: 331, x: 360, time_counter: 585)\nCoordinates:\n    nav_lat        (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    nav_lon        (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_centered  (time_counter) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time_counter   (time_counter) datetime64[ns] 5kB 1976-01-16T12:00:00 ... ...\nDimensions without coordinates: y, x\nData variables:\n    tos_con        (time_counter, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>y: 331</li><li>x: 360</li><li>time_counter: 585</li></ul></li><li>Coordinates: (4)<ul><li>nav_lat(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>nav_lon(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>time_centered(time_counter)datetime64[ns]dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;bounds :time_centered_boundslong_name :Time axisstandard_name :timetime_origin :1900-01-01 00:00:00  Array   Chunk   Bytes   4.57 kiB   8 B   Shape   (585,)   (1,)   Dask graph   585 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  585 1 </li><li>time_counter(time_counter)datetime64[ns]1976-01-16T12:00:00 ... 2024-09-16axis :Tbounds :time_counter_boundslong_name :Time axisstandard_name :timetime_origin :1900-01-01 00:00:00<pre>array(['1976-01-16T12:00:00.000000000', '1976-02-15T12:00:00.000000000',\n       '1976-03-16T12:00:00.000000000', ..., '2024-07-16T12:00:00.000000000',\n       '2024-08-16T12:00:00.000000000', '2024-09-16T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>tos_con(time_counter, y, x)float32dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;cell_methods :time: mean (interval: 3600 s)interval_operation :3600 sinterval_write :1 monthlong_name :sea_surface_conservative_temperatureonline_operation :averagestandard_name :sea_surface_temperatureunits :degC  Array   Chunk   Bytes   265.92 MiB   465.47 kiB   Shape   (585, 331, 360)   (1, 331, 360)   Dask graph   585 chunks in 2 graph layers   Data type   float32 numpy.ndarray  360 331 585 </li></ul></li><li>Indexes: (1)<ul><li>time_counterPandasIndex<pre>PandasIndex(DatetimeIndex(['1976-01-16 12:00:00', '1976-02-15 12:00:00',\n               '1976-03-16 12:00:00', '1976-04-16 00:00:00',\n               '1976-05-16 12:00:00', '1976-06-16 00:00:00',\n               '1976-07-16 12:00:00', '1976-08-16 12:00:00',\n               '1976-09-16 00:00:00', '1976-10-16 12:00:00',\n               ...\n               '2023-12-16 12:00:00', '2024-01-16 12:00:00',\n               '2024-02-15 12:00:00', '2024-03-16 12:00:00',\n               '2024-04-16 00:00:00', '2024-05-16 12:00:00',\n               '2024-06-16 00:00:00', '2024-07-16 12:00:00',\n               '2024-08-16 12:00:00', '2024-09-16 00:00:00'],\n              dtype='datetime64[ns]', name='time_counter', length=585, freq=None))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[3]: Copied! <pre># Updating model coords:\nds_tos_con = ds_tos_con.rename({'nav_lat': 'lat', 'nav_lon': 'lon', 'time_counter': 'time'})\n\n# Adding a 2-D land/ocean mask to our Dataset:\nds_tos_con['mask'] = ds_domain['tmaskutil'].squeeze()\n\nds_tos_con['mask'].plot()\n</pre> # Updating model coords: ds_tos_con = ds_tos_con.rename({'nav_lat': 'lat', 'nav_lon': 'lon', 'time_counter': 'time'})  # Adding a 2-D land/ocean mask to our Dataset: ds_tos_con['mask'] = ds_domain['tmaskutil'].squeeze()  ds_tos_con['mask'].plot() Out[3]: <pre>&lt;matplotlib.collections.QuadMesh at 0x14a1618dc920&gt;</pre> In\u00a0[4]: Copied! <pre># Creating an empty ModelValidator object:\nmv = ModelValidator(mdl_data=ds_tos_con)\nmv\n</pre> # Creating an empty ModelValidator object: mv = ModelValidator(mdl_data=ds_tos_con) mv Out[4]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 281MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    lon            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    tos_con        (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*</pre> In\u00a0[5]: Copied! <pre># Accessing our original ocean model data:\nmv.data\n</pre> # Accessing our original ocean model data: mv.data Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 281MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    lon            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    tos_con        (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>y: 331</li><li>x: 360</li><li>time: 585</li></ul></li><li>Coordinates: (5)<ul><li>lat(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>lon(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>time_centered(time)datetime64[ns]dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;bounds :time_centered_boundslong_name :Time axisstandard_name :timetime_origin :1900-01-01 00:00:00  Array   Chunk   Bytes   4.57 kiB   8 B   Shape   (585,)   (1,)   Dask graph   585 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  585 1 </li><li>time(time)datetime64[ns]1976-01-16T12:00:00 ... 2024-09-16axis :Tbounds :time_counter_boundslong_name :Time axisstandard_name :timetime_origin :1900-01-01 00:00:00<pre>array(['1976-01-16T12:00:00.000000000', '1976-02-15T12:00:00.000000000',\n       '1976-03-16T12:00:00.000000000', ..., '2024-07-16T12:00:00.000000000',\n       '2024-08-16T12:00:00.000000000', '2024-09-16T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_counter()float320.0<pre>array(0., dtype=float32)</pre></li></ul></li><li>Data variables: (2)<ul><li>tos_con(time, y, x)float32dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;cell_methods :time: mean (interval: 3600 s)interval_operation :3600 sinterval_write :1 monthlong_name :sea_surface_conservative_temperatureonline_operation :averagestandard_name :sea_surface_temperatureunits :degC  Array   Chunk   Bytes   265.92 MiB   465.47 kiB   Shape   (585, 331, 360)   (1, 331, 360)   Dask graph   585 chunks in 2 graph layers   Data type   float32 numpy.ndarray  360 331 585 </li><li>mask(y, x)int8dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   116.37 kiB   116.37 kiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 3 graph layers   Data type   int8 numpy.ndarray  360 331 </li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1976-01-16 12:00:00', '1976-02-15 12:00:00',\n               '1976-03-16 12:00:00', '1976-04-16 00:00:00',\n               '1976-05-16 12:00:00', '1976-06-16 00:00:00',\n               '1976-07-16 12:00:00', '1976-08-16 12:00:00',\n               '1976-09-16 00:00:00', '1976-10-16 12:00:00',\n               ...\n               '2023-12-16 12:00:00', '2024-01-16 12:00:00',\n               '2024-02-15 12:00:00', '2024-03-16 12:00:00',\n               '2024-04-16 00:00:00', '2024-05-16 12:00:00',\n               '2024-06-16 00:00:00', '2024-07-16 12:00:00',\n               '2024-08-16 12:00:00', '2024-09-16 00:00:00'],\n              dtype='datetime64[ns]', name='time', length=585, freq=None))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[6]: Copied! <pre>ModelValidator()\n</pre> ModelValidator() Out[6]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*</pre>"},{"location":"ex1_getting_started/#getting-started-with-validocean","title":"Getting Started with ValidOcean\u00b6","text":""},{"location":"ex1_getting_started/#description","title":"Description:\u00b6","text":"<p>This tutorial notebook introduces the ModelValidator Class used for vaidating ocean model outputs &amp; accessing ocean observations via cloud object storage.</p> <p>To demonstrate the basic features of the ModelValidator Class, we will use outputs from the National Oceanography Centre's Near-Present-Day global 1-degree (eORCA1-NPD-ERA5v1) ocean sea-ice simulation, which represents the historical period from 1976 - present.</p> <p>For more details on this model configuration and the available outputs, users should explore the Near-Present-Day documentation here.</p>"},{"location":"ex1_getting_started/#contact","title":"Contact:\u00b6","text":"<p>Ollie Tooth (oliver.tooth@noc.ac.uk)</p>"},{"location":"ex1_getting_started/#accessing-example-ocean-model-data","title":"Accessing Example Ocean Model Data\u00b6","text":"<p>We will begin by using the xarray Python library to load an example dataset from the eORCA1-NPD-ERA5v1 outputs available on the JASMIN Object Store.</p> <p>To start, we will load the NEMO ocean model domain variables and then we will load monthly mean sea surface temperatures for the global domain...</p>"},{"location":"ex1_getting_started/#preparing-example-ocean-model-data","title":"Preparing Example Ocean Model Data\u00b6","text":"<p>Next, we need to update our ocean model coordinate variables to conform to the standard names used by ValidOcean.</p> <p>We also need to add a mask file distinguishing between land and ocean grid cells in the model domain.</p>"},{"location":"ex1_getting_started/#creating-a-modelvalidator","title":"Creating A ModelValidator\u00b6","text":"<p>The ModelValidator is the core class of the ValidOcean library, giving users access to methods designed to accelerate the validation of ocean general circulation models &amp; a range of ocean observations stored in the cloud.</p> <p>To get started, let's create an instance of ModelValidator() class using our example ocean model outputs...</p>"},{"location":"ex1_getting_started/#components-of-a-modelvalidator","title":"Components of a ModelValidator\u00b6","text":"<p>Following the creation of an empty ModelValidator, the above output shows us that a Model Validator object is simply a container for many xarray Datasets which we have yet to populate with any data!</p> <p>There are four core attributes of each ModelValidator object:</p> <ol> <li><p>Model Data: A Dataset storing the original ocean model data that we provided as input.</p> </li> <li><p>Observations: An empty Dataset to store any ocean observations that we load or use in our validation workflow.</p> </li> <li><p>Results: An empty Dataset to store any results of our validation workflows (e.g., model - observation errors).</p> </li> <li><p>Statistics: An empty Dataset to store any aggregated statistics resulting from our validation workflows (e.g., RMSE of model - observation errors).</p> </li> </ol> <p>To access any core attributes of our ModelValidator, we can simply use its corresponding attribute...</p>"},{"location":"ex1_getting_started/#creating-an-empty-modelvalidator","title":"Creating an Empty ModelValidator\u00b6","text":"<p>In addition to providing many reusable validation workflows (see below), ModelValidator objects can also be used as an interface to ocean observations stored in the cloud.</p> <p>When accessing or compare ocean observations, we can also create an empty ModelValidator without passing any ocean model outputs as follows...</p>"},{"location":"ex1_getting_started/#next-steps","title":"Next Steps...\u00b6","text":"<p>In this tutorial, we have seen how to create a ModelValidator object and introduced its core components.</p> <p>Next, we will introduce validation workflows using our example sea surface temperature dataset in the ex2_sst_validation_workflow.ipynb notebook.</p>"},{"location":"ex2_sst_validation_workflow/","title":"SST Validation Workflows with ValidOcean","text":"In\u00a0[1]: Copied! <pre># -- Import required libraries -- #\nimport xarray as xr\nfrom ValidOcean import ModelValidator\n</pre> # -- Import required libraries -- # import xarray as xr from ValidOcean import ModelValidator <pre>/home/otooth/miniconda3/envs/env_nemo_validation/lib/python3.12/site-packages/esmpy/interface/loadESMF.py:94: VersionWarning: ESMF installation version 8.8.0, ESMPy version 8.8.0b0\n  warnings.warn(\"ESMF installation version {}, ESMPy version {}\".format(\n</pre> In\u00a0[2]: Copied! <pre># Define url to domain store:\nurl_domain = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/domain/\"\nds_domain = xr.open_zarr(url_domain, consolidated=True)\n\n# Define url to sea surface temperature (tos_con) store:\nurl_tos_con = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/T1m/tos_con/\"\nds_tos_con = xr.open_zarr(url_tos_con, consolidated=True)\n\n# Updating model coords:\nds_tos_con = ds_tos_con.rename({'nav_lat': 'lat', 'nav_lon': 'lon', 'time_counter': 'time'})\n# Adding a 2-D land/ocean mask to our Dataset:\nds_tos_con['mask'] = ds_domain['tmaskutil'].squeeze()\n\nds_tos_con\n</pre> # Define url to domain store: url_domain = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/domain/\" ds_domain = xr.open_zarr(url_domain, consolidated=True)  # Define url to sea surface temperature (tos_con) store: url_tos_con = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/T1m/tos_con/\" ds_tos_con = xr.open_zarr(url_tos_con, consolidated=True)  # Updating model coords: ds_tos_con = ds_tos_con.rename({'nav_lat': 'lat', 'nav_lon': 'lon', 'time_counter': 'time'}) # Adding a 2-D land/ocean mask to our Dataset: ds_tos_con['mask'] = ds_domain['tmaskutil'].squeeze()  ds_tos_con Out[2]: <pre>&lt;xarray.Dataset&gt; Size: 281MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    lon            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    tos_con        (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>y: 331</li><li>x: 360</li><li>time: 585</li></ul></li><li>Coordinates: (5)<ul><li>lat(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>lon(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>time_centered(time)datetime64[ns]dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;bounds :time_centered_boundslong_name :Time axisstandard_name :timetime_origin :1900-01-01 00:00:00  Array   Chunk   Bytes   4.57 kiB   8 B   Shape   (585,)   (1,)   Dask graph   585 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  585 1 </li><li>time(time)datetime64[ns]1976-01-16T12:00:00 ... 2024-09-16axis :Tbounds :time_counter_boundslong_name :Time axisstandard_name :timetime_origin :1900-01-01 00:00:00<pre>array(['1976-01-16T12:00:00.000000000', '1976-02-15T12:00:00.000000000',\n       '1976-03-16T12:00:00.000000000', ..., '2024-07-16T12:00:00.000000000',\n       '2024-08-16T12:00:00.000000000', '2024-09-16T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_counter()float320.0<pre>array(0., dtype=float32)</pre></li></ul></li><li>Data variables: (2)<ul><li>tos_con(time, y, x)float32dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;cell_methods :time: mean (interval: 3600 s)interval_operation :3600 sinterval_write :1 monthlong_name :sea_surface_conservative_temperatureonline_operation :averagestandard_name :sea_surface_temperatureunits :degC  Array   Chunk   Bytes   265.92 MiB   465.47 kiB   Shape   (585, 331, 360)   (1, 331, 360)   Dask graph   585 chunks in 2 graph layers   Data type   float32 numpy.ndarray  360 331 585 </li><li>mask(y, x)int8dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   116.37 kiB   116.37 kiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 3 graph layers   Data type   int8 numpy.ndarray  360 331 </li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1976-01-16 12:00:00', '1976-02-15 12:00:00',\n               '1976-03-16 12:00:00', '1976-04-16 00:00:00',\n               '1976-05-16 12:00:00', '1976-06-16 00:00:00',\n               '1976-07-16 12:00:00', '1976-08-16 12:00:00',\n               '1976-09-16 00:00:00', '1976-10-16 12:00:00',\n               ...\n               '2023-12-16 12:00:00', '2024-01-16 12:00:00',\n               '2024-02-15 12:00:00', '2024-03-16 12:00:00',\n               '2024-04-16 00:00:00', '2024-05-16 12:00:00',\n               '2024-06-16 00:00:00', '2024-07-16 12:00:00',\n               '2024-08-16 12:00:00', '2024-09-16 00:00:00'],\n              dtype='datetime64[ns]', name='time', length=585, freq=None))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[\u00a0]: Copied! <pre># Creating a new ModelValidator object:\nmv = ModelValidator(mdl_data=ds_tos_con)\nmv\n</pre> # Creating a new ModelValidator object: mv = ModelValidator(mdl_data=ds_tos_con) mv Out[\u00a0]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 281MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    lon            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    tos_con        (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*</pre> In\u00a0[4]: Copied! <pre>mv.plot_sst_error(sst_name='tos_con',\n                  obs_name='OISSTv2',\n                  time_bounds='1991-2020',\n                  freq='total',\n                  regrid_to='model',\n                  method='bilinear',\n                  source_plots=True,\n                  stats=True,\n                  )\n</pre> mv.plot_sst_error(sst_name='tos_con',                   obs_name='OISSTv2',                   time_bounds='1991-2020',                   freq='total',                   regrid_to='model',                   method='bilinear',                   source_plots=True,                   stats=True,                   ) Out[4]: <pre>array([&lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;], dtype=object)</pre> In\u00a0[5]: Copied! <pre>mv.obs\n</pre> mv.obs Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:               (y: 331, x: 360)\nCoordinates:\n    lat_oisstv2           (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    lon_oisstv2           (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_counter_oisstv2  float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    tos_con_oisstv2       (y, x) float32 477kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>y: 331</li><li>x: 360</li></ul></li><li>Coordinates: (3)<ul><li>lat_oisstv2(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>lon_oisstv2(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>time_counter_oisstv2()float320.0<pre>array(0., dtype=float32)</pre></li></ul></li><li>Data variables: (1)<ul><li>tos_con_oisstv2(y, x)float32dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;regrid_method :bilinear  Array   Chunk   Bytes   465.47 kiB   465.47 kiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 15 graph layers   Data type   float32 numpy.ndarray  360 331 </li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> <p>We can see that the model - observation SST error (<code>tos_con_error</code> shown in panel (c) above) has been included alongside the original ocean model SST <code>tos_con</code> in the <code>.results</code> attribute.</p> In\u00a0[6]: Copied! <pre>mv.results\n</pre> mv.results Out[6]: <pre>&lt;xarray.Dataset&gt; Size: 3MB\nDimensions:        (obs: 1, y: 331, x: 360)\nCoordinates:\n  * obs            (obs) object 8B 'OISSTv2'\n    lat            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    lon            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    tos_con_error  (obs, y, x) float32 477kB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    tos_con        (y, x) float32 477kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs: 1</li><li>y: 331</li><li>x: 360</li></ul></li><li>Coordinates: (4)<ul><li>obs(obs)object'OISSTv2'<pre>array(['OISSTv2'], dtype=object)</pre></li><li>lat(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>lon(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>time_counter()float320.0<pre>array(0., dtype=float32)</pre></li></ul></li><li>Data variables: (2)<ul><li>tos_con_error(obs, y, x)float32dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   465.47 kiB   465.47 kiB   Shape   (1, 331, 360)   (1, 331, 360)   Dask graph   1 chunks in 26 graph layers   Data type   float32 numpy.ndarray  360 331 1 </li><li>tos_con(y, x)float32dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   465.47 kiB   465.47 kiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 9 graph layers   Data type   float32 numpy.ndarray  360 331 </li></ul></li><li>Indexes: (1)<ul><li>obsPandasIndex<pre>PandasIndex(Index(['OISSTv2'], dtype='object', name='obs'))</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>Finally, we can also compute the aggregate statistics quantifying the error between our ocean model &amp; the OISSTv2 SST climatology by calling <code>.compute()</code> on the <code>.stats</code> attribute.</p> In\u00a0[7]: Copied! <pre>mv.stats.compute()\n</pre> mv.stats.compute() Out[7]: <pre>&lt;xarray.Dataset&gt; Size: 16B\nDimensions:                 ()\nCoordinates:\n    time_counter            float32 4B 0.0\nData variables:\n    Mean Absolute Error     float32 4B 0.3241\n    Mean Square Error       float32 4B 0.2731\n    Root Mean Square Error  float32 4B 0.5226</pre>xarray.Dataset<ul><li>Dimensions:</li><li>Coordinates: (1)<ul><li>time_counter()float320.0<pre>array(0., dtype=float32)</pre></li></ul></li><li>Data variables: (3)<ul><li>Mean Absolute Error()float320.3241<pre>array(0.32411742, dtype=float32)</pre></li><li>Mean Square Error()float320.2731<pre>array(0.2731251, dtype=float32)</pre></li><li>Root Mean Square Error()float320.5226<pre>array(0.5226137, dtype=float32)</pre></li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> In\u00a0[8]: Copied! <pre># Creating a regional ModelValidator object:\nmv_regional = ModelValidator(mdl_data=ds_tos_con.isel(x=slice(200, 300), y=slice(220, None)))\n</pre> # Creating a regional ModelValidator object: mv_regional = ModelValidator(mdl_data=ds_tos_con.isel(x=slice(200, 300), y=slice(220, None))) In\u00a0[9]: Copied! <pre>mv_regional.compute_sst_error(sst_name='tos_con',\n                              obs_name='OISSTv2',\n                              time_bounds='1991-2020',\n                              freq='seasonal',\n                              regrid_to='model',\n                              method='bilinear',\n                              stats=True,\n                              )\n</pre> mv_regional.compute_sst_error(sst_name='tos_con',                               obs_name='OISSTv2',                               time_bounds='1991-2020',                               freq='seasonal',                               regrid_to='model',                               method='bilinear',                               stats=True,                               ) Out[9]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 26MB\nDimensions:        (y: 111, x: 100, time: 585)\nCoordinates:\n    lat            (y, x) float64 89kB dask.array&lt;chunksize=(111, 100), meta=np.ndarray&gt;\n    lon            (y, x) float64 89kB dask.array&lt;chunksize=(111, 100), meta=np.ndarray&gt;\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    tos_con        (time, y, x) float32 26MB dask.array&lt;chunksize=(1, 111, 100), meta=np.ndarray&gt;\n    mask           (y, x) int8 11kB dask.array&lt;chunksize=(111, 100), meta=np.ndarray&gt;\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 355kB\nDimensions:               (season_oisstv2: 4, y: 111, x: 100)\nCoordinates:\n  * season_oisstv2        (season_oisstv2) object 32B 'DJF' 'JJA' 'MAM' 'SON'\n    lat_oisstv2           (y, x) float64 89kB dask.array&lt;chunksize=(111, 100), meta=np.ndarray&gt;\n    lon_oisstv2           (y, x) float64 89kB dask.array&lt;chunksize=(111, 100), meta=np.ndarray&gt;\n    time_counter_oisstv2  float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    tos_con_oisstv2       (season_oisstv2, y, x) float32 178kB dask.array&lt;chunksize=(4, 111, 100), meta=np.ndarray&gt;\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 533kB\nDimensions:        (obs: 1, y: 111, x: 100, season: 4)\nCoordinates:\n  * obs            (obs) object 8B 'OISSTv2'\n    lat            (y, x) float64 89kB dask.array&lt;chunksize=(111, 100), meta=np.ndarray&gt;\n    lon            (y, x) float64 89kB dask.array&lt;chunksize=(111, 100), meta=np.ndarray&gt;\n    time_counter   float32 4B 0.0\n  * season         (season) object 32B 'DJF' 'JJA' 'MAM' 'SON'\nDimensions without coordinates: y, x\nData variables:\n    tos_con_error  (obs, season, y, x) float32 178kB dask.array&lt;chunksize=(1, 1, 111, 100), meta=np.ndarray&gt;\n    tos_con        (season, y, x) float32 178kB dask.array&lt;chunksize=(1, 111, 100), meta=np.ndarray&gt;\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 84B\nDimensions:                 (season: 4)\nCoordinates:\n    time_counter            float32 4B 0.0\n  * season                  (season) object 32B 'DJF' 'JJA' 'MAM' 'SON'\nData variables:\n    Mean Absolute Error     (season) float32 16B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    Mean Square Error       (season) float32 16B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    Root Mean Square Error  (season) float32 16B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;</pre> <p>If we did want to visualise the results of our validation workflow, we could modify our previous example of the <code>plot_sst_error</code> method as follows..</p> In\u00a0[10]: Copied! <pre>mv_regional.plot_sst_error(sst_name='tos_con',\n                           obs_name='OISSTv2',\n                           time_bounds='1991-2020',\n                           freq='seasonal',\n                           regrid_to='model',\n                           method='bilinear',\n                           stats=True,\n                           )\n</pre> mv_regional.plot_sst_error(sst_name='tos_con',                            obs_name='OISSTv2',                            time_bounds='1991-2020',                            freq='seasonal',                            regrid_to='model',                            method='bilinear',                            stats=True,                            ) <pre>/dssgfs01/working/otooth/Diagnostics/ValidOcean/ValidOcean/plotting.py:77: RuntimeWarning: source_plots = True is only available where freq = 'total'. Using source_plots = False.\n  warnings.warn(\"source_plots = True is only available where freq = 'total'. Using source_plots = False.\", RuntimeWarning)\n</pre> Out[10]: <pre>array([&lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;], dtype=object)</pre>"},{"location":"ex2_sst_validation_workflow/#sst-validation-workflows-with-validocean","title":"SST Validation Workflows with ValidOcean\u00b6","text":""},{"location":"ex2_sst_validation_workflow/#description","title":"Description:\u00b6","text":"<p>This tutorial notebook introduces a typical sea surface temperature validation workflow in ValidOcean using the ModelValidator class.</p> <p>To demonstrate a typical sea surface temperature validation workflow, we will use outputs from the National Oceanography Centre's Near-Present-Day global 1-degree (eORCA1-NPD-ERA5v1) ocean sea-ice simulation introduced in Example 1, which represents the historical period from 1976 - present.</p> <p>For more details on this model configuration and the available outputs, users should explore the Near-Present-Day documentation here.</p>"},{"location":"ex2_sst_validation_workflow/#contact","title":"Contact:\u00b6","text":"<p>Ollie Tooth (oliver.tooth@noc.ac.uk)</p>"},{"location":"ex2_sst_validation_workflow/#accessing-preparing-example-ocean-model-data","title":"Accessing &amp; Preparing Example Ocean Model Data\u00b6","text":"<p>We will begin by using the xarray Python library to load an example dataset from the eORCA1-NPD-ERA5v1 outputs available on the JASMIN Object Store.</p> <p>As in Example 1, we will load the NEMO ocean model domain variables and then we will load monthly mean sea surface temperatures for the global domain.</p> <p>Before creating a <code>ModelValidator()</code> object, we need to update our ocean model coordinate variables to conform to the standard names used by ValidOcean &amp; add a land/ocean mask file for regridding.</p>"},{"location":"ex2_sst_validation_workflow/#creating-a-modelvalidator","title":"Creating A ModelValidator\u00b6","text":"<p>Now let's create a new <code>ModelValidator()</code> object using our monthly sea surface temperature data.</p>"},{"location":"ex2_sst_validation_workflow/#plotting-model-observations-sea-surface-temperature-bias","title":"Plotting (Model - Observations) Sea Surface Temperature Bias\u00b6","text":"<p>Consider the typical scenario where we have just completed a ocean model simulation and would now like to know how well the surface properties in our hindcast represent the real ocean. A common starting point is to calculate the difference or bias between the climatologies of sea surface temperature (SST) simulated in the model and recorded in observations.</p> <p>We can divide this validation workflow into several steps:</p> <ol> <li>Select &amp; load our ocean observation dataset</li> </ol> <ul> <li>We will use the 1991-2020 climatology of the NOAA High-resolution Blended Analysis of Daily SST (OISSTv2) product.</li> </ul> <ol> <li>Load &amp; subset our ocean model dataset</li> </ol> <ul> <li>Only a temporal subsetting is needed to match the observations since OISSTv2 has global coverage.</li> </ul> <ol> <li><p>Calculate the SST climatology from our ocean model dataset.</p> </li> <li><p>Regrid either the model climatology onto the observations grid or vice versa</p> </li> </ol> <ul> <li>We regrid the observations climatology onto the model grid using bilinar interpolation.</li> </ul> <ol> <li>Calculate the (model - observation) error &amp; aggregated statistics</li> </ol> <ul> <li>We return the MAE, MSE and RMSE.</li> </ul> <ol> <li>Plot the (model - observation) SST error</li> </ol> <ul> <li>We also include the model &amp; observation SST climatologies.</li> </ul> <p>The true value of ValidOcean is that all of the steps are perfomed for you by using the plot_sst_error() method of the ModelValidator...</p>"},{"location":"ex2_sst_validation_workflow/#results-of-our-validation-workflow","title":"Results of our Validation Workflow\u00b6","text":"<p>In addition to the plot shown above, calling the <code>plot_sst_error()</code> method has populated the <code>.obs</code>, <code>.results</code> and <code>.stats</code> attributes with the results of our validation workflow.</p> <p>Let's start by exploring the observational data which has been added to the <code>.obs</code> attribute.</p> <p>Here, we can see that a new DataArray <code>tos_con_oisstv2</code> has been added to the observations dataset as a result of regridding the OISSTv2 (1991-2020) climatology onto our original model grid using bilinear interpolation (shown in panel (b) above).</p>"},{"location":"ex2_sst_validation_workflow/#validation-without-visualisation","title":"Validation Without Visualisation\u00b6","text":"<p>To perform our SST validation workflow without visualising the results, we can alternatively use the <code>.compute_sst_error()</code> method, which returns all of the results discussed above without the matplotlib axes object.</p> <p>To demonstrate this method, we will create a new <code>ModelValidator</code> object using a regional North Atlantic subdomain of our ocean model dataset and perform a seasonal SST validation workflow as follows...</p>"},{"location":"ex2_sst_validation_workflow/#next-steps","title":"Next Steps...\u00b6","text":"<p>In this tutorial, we have seen how to perform a typical ocean model validation worfklow for sea surface temperature data using the ModelValidator object.</p> <p>Next, we will explore how to use the ModelValidator to validate sea ice data, including aggregated diagnostics (e.g., sea ice area), in the ex3_sea_ice_validation_workflow.ipynb notebook.</p>"},{"location":"ex3_sea_ice_validation_workflow/","title":"Sea Ice Validation Workflows with ValidOcean","text":"In\u00a0[1]: Copied! <pre># -- Import required libraries -- #\nimport xarray as xr\nfrom ValidOcean import ModelValidator\n</pre> # -- Import required libraries -- # import xarray as xr from ValidOcean import ModelValidator <pre>/home/otooth/miniconda3/envs/env_nemo_validation/lib/python3.12/site-packages/esmpy/interface/loadESMF.py:94: VersionWarning: ESMF installation version 8.8.0, ESMPy version 8.8.0b0\n  warnings.warn(\"ESMF installation version {}, ESMPy version {}\".format(\n</pre> In\u00a0[2]: Copied! <pre># Define url to domain store:\nurl_domain = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/domain/\"\nds_domain = xr.open_zarr(url_domain, consolidated=True)\n\n# Define url to sea surface temperature (tos_con) store:\nurl_siconc = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/I1m/siconc/\"\nds_siconc = xr.open_zarr(url_siconc, consolidated=True)\n\n# Updating model coords:\nds_siconc = ds_siconc.rename({'nav_lat': 'lat', 'nav_lon': 'lon', 'time_counter': 'time'})\n# Adding a 2-D land/ocean mask to our Dataset:\nds_siconc['mask'] = ds_domain['tmaskutil'].squeeze()\n\nds_siconc\n</pre> # Define url to domain store: url_domain = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/domain/\" ds_domain = xr.open_zarr(url_domain, consolidated=True)  # Define url to sea surface temperature (tos_con) store: url_siconc = \"https://noc-msm-o.s3-ext.jc.rl.ac.uk/npd-eorca1-era5v1/I1m/siconc/\" ds_siconc = xr.open_zarr(url_siconc, consolidated=True)  # Updating model coords: ds_siconc = ds_siconc.rename({'nav_lat': 'lat', 'nav_lon': 'lon', 'time_counter': 'time'}) # Adding a 2-D land/ocean mask to our Dataset: ds_siconc['mask'] = ds_domain['tmaskutil'].squeeze()  ds_siconc Out[2]: <pre>&lt;xarray.Dataset&gt; Size: 281MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    lon            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    siconc         (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>y: 331</li><li>x: 360</li><li>time: 585</li></ul></li><li>Coordinates: (5)<ul><li>lat(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>lon(y, x)float64dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.91 MiB   0.91 MiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  360 331 </li><li>time_centered(time)datetime64[ns]dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;bounds :time_centered_boundslong_name :Time axisstandard_name :timetime_origin :1900-01-01 00:00:00  Array   Chunk   Bytes   4.57 kiB   8 B   Shape   (585,)   (1,)   Dask graph   585 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  585 1 </li><li>time(time)datetime64[ns]1976-01-16T12:00:00 ... 2024-09-16axis :Tbounds :time_counter_boundslong_name :Time axisstandard_name :timetime_origin :1900-01-01 00:00:00<pre>array(['1976-01-16T12:00:00.000000000', '1976-02-15T12:00:00.000000000',\n       '1976-03-16T12:00:00.000000000', ..., '2024-07-16T12:00:00.000000000',\n       '2024-08-16T12:00:00.000000000', '2024-09-16T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_counter()float320.0<pre>array(0., dtype=float32)</pre></li></ul></li><li>Data variables: (2)<ul><li>siconc(time, y, x)float32dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;cell_methods :time: mean (interval: 3600 s)interval_operation :3600 sinterval_write :1 monthlong_name :Sea-ice area fractiononline_operation :averagestandard_name :sea_ice_area_fractionunits :  Array   Chunk   Bytes   265.92 MiB   465.47 kiB   Shape   (585, 331, 360)   (1, 331, 360)   Dask graph   585 chunks in 2 graph layers   Data type   float32 numpy.ndarray  360 331 585 </li><li>mask(y, x)int8dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   116.37 kiB   116.37 kiB   Shape   (331, 360)   (331, 360)   Dask graph   1 chunks in 3 graph layers   Data type   int8 numpy.ndarray  360 331 </li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1976-01-16 12:00:00', '1976-02-15 12:00:00',\n               '1976-03-16 12:00:00', '1976-04-16 00:00:00',\n               '1976-05-16 12:00:00', '1976-06-16 00:00:00',\n               '1976-07-16 12:00:00', '1976-08-16 12:00:00',\n               '1976-09-16 00:00:00', '1976-10-16 12:00:00',\n               ...\n               '2023-12-16 12:00:00', '2024-01-16 12:00:00',\n               '2024-02-15 12:00:00', '2024-03-16 12:00:00',\n               '2024-04-16 00:00:00', '2024-05-16 12:00:00',\n               '2024-06-16 00:00:00', '2024-07-16 12:00:00',\n               '2024-08-16 12:00:00', '2024-09-16 00:00:00'],\n              dtype='datetime64[ns]', name='time', length=585, freq=None))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[3]: Copied! <pre># Creating a new ModelValidator object:\nmv = ModelValidator(mdl_data=ds_siconc)\nmv\n</pre> # Creating a new ModelValidator object: mv = ModelValidator(mdl_data=ds_siconc) mv Out[3]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 281MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    lon            (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    siconc         (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*</pre> In\u00a0[4]: Copied! <pre>mv.plot_siconc_error(sic_name='siconc',\n                     obs_name='NSIDC',\n                     region='arctic',\n                     time_bounds=slice('2001-01', '2020-12'),\n                     figsize=(15,12),\n                     freq='mar',\n                     regrid_to='model',\n                     method='bilinear',\n                     source_plots=True,\n                     stats=True,\n                     )\n</pre> mv.plot_siconc_error(sic_name='siconc',                      obs_name='NSIDC',                      region='arctic',                      time_bounds=slice('2001-01', '2020-12'),                      figsize=(15,12),                      freq='mar',                      regrid_to='model',                      method='bilinear',                      source_plots=True,                      stats=True,                      ) <pre>/dssgfs01/working/otooth/Diagnostics/ValidOcean/ValidOcean/processing.py:208: RuntimeWarning: [longitude: -180.0, 180.0; latitude: -86.0, 90.0] bounds are outside the range of available observations data [longitude: -180.0, 179.81397539549246; latitude: 31.102671752463447, 89.83681599961737].\n  warnings.warn(warning_message, RuntimeWarning)\n</pre> Out[4]: <pre>array([&lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;], dtype=object)</pre> In\u00a0[5]: Copied! <pre>mv.obs\n</pre> mv.obs Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 838kB\nDimensions:             (y: 97, x: 360)\nCoordinates:\n    band_nsidc          int64 8B 1\n    spatial_ref_nsidc   int64 8B 0\n    lat_nsidc           (y, x) float64 279kB 30.86 30.86 30.86 ... 50.23 50.01\n    lon_nsidc           (y, x) float64 279kB 73.5 74.5 75.5 ... 72.96 72.99\n    time_counter_nsidc  float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    siconc_nsidc        (y, x) float64 279kB dask.array&lt;chunksize=(97, 360), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>y: 97</li><li>x: 360</li></ul></li><li>Coordinates: (5)<ul><li>band_nsidc()int641<pre>array(1)</pre></li><li>spatial_ref_nsidc()int640<pre>array(0)</pre></li><li>lat_nsidc(y, x)float6430.86 30.86 30.86 ... 50.23 50.01<pre>array([[30.85618361, 30.85643034, 30.85692351, ..., 30.85692351,\n        30.85643034, 30.85618361],\n       [31.6022862 , 31.60259023, 31.60319794, ..., 31.60319794,\n        31.60259023, 31.6022862 ],\n       [32.32770907, 32.32807786, 32.32881504, ..., 32.32881504,\n        32.32807786, 32.32770907],\n       ...,\n       [49.99324171, 50.21420708, 50.49226259, ..., 50.49226259,\n        50.21420708, 49.99324171],\n       [50.00501891, 50.22440337, 50.50234643, ..., 50.50234643,\n        50.22440337, 50.00501891],\n       [50.01094032, 50.22936197, 50.50726479, ..., 50.50726479,\n        50.22936197, 50.01094032]])</pre></li><li>lon_nsidc(y, x)float6473.5 74.5 75.5 ... 72.96 72.99<pre>array([[73.49956612, 74.49869878, 75.49783265, ..., 70.50216735,\n        71.50130122, 72.50043388],\n       [73.49942579, 74.49827789, 75.49713157, ..., 70.50286843,\n        71.50172211, 72.50057421],\n       [73.49925634, 74.49776967, 75.496285  , ..., 70.503715  ,\n        71.50223033, 72.50074366],\n       ...,\n       [73.07927918, 73.19734462, 73.28416978, ..., 72.71583022,\n        72.80265538, 72.92072082],\n       [73.0423483 , 73.11531867, 73.16770504, ..., 72.83229496,\n        72.88468133, 72.9576517 ],\n       [73.01084559, 73.03703107, 73.05471492, ..., 72.94528508,\n        72.96296893, 72.98915441]])</pre></li><li>time_counter_nsidc()float320.0<pre>array(0., dtype=float32)</pre></li></ul></li><li>Data variables: (1)<ul><li>siconc_nsidc(y, x)float64dask.array&lt;chunksize=(97, 360), meta=np.ndarray&gt;regrid_method :bilinear  Array   Chunk   Bytes   272.81 kiB   272.81 kiB   Shape   (97, 360)   (97, 360)   Dask graph   1 chunks in 15 graph layers   Data type   float64 numpy.ndarray  360 97 </li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> <p>We can see that the model - observation SIC error (<code>siconc_error</code> shown in panel (c) above) has been included alongside the original ocean model SIC <code>siconc</code> in the <code>.results</code> attribute.</p> In\u00a0[6]: Copied! <pre>mv.results\n</pre> mv.results Out[6]: <pre>&lt;xarray.Dataset&gt; Size: 978kB\nDimensions:       (obs: 1, y: 97, x: 360)\nCoordinates:\n  * obs           (obs) object 8B 'NSIDC'\n    lat           (y, x) float64 279kB 30.86 30.86 30.86 ... 50.51 50.23 50.01\n    lon           (y, x) float64 279kB 73.5 74.5 75.5 76.5 ... 72.95 72.96 72.99\n    time_counter  float32 4B 0.0\n    band          int64 8B 1\n    spatial_ref   int64 8B 0\nDimensions without coordinates: y, x\nData variables:\n    siconc_error  (obs, y, x) float64 279kB dask.array&lt;chunksize=(1, 97, 360), meta=np.ndarray&gt;\n    siconc        (y, x) float32 140kB dask.array&lt;chunksize=(97, 360), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs: 1</li><li>y: 97</li><li>x: 360</li></ul></li><li>Coordinates: (6)<ul><li>obs(obs)object'NSIDC'<pre>array(['NSIDC'], dtype=object)</pre></li><li>lat(y, x)float6430.86 30.86 30.86 ... 50.23 50.01<pre>array([[30.85618361, 30.85643034, 30.85692351, ..., 30.85692351,\n        30.85643034, 30.85618361],\n       [31.6022862 , 31.60259023, 31.60319794, ..., 31.60319794,\n        31.60259023, 31.6022862 ],\n       [32.32770907, 32.32807786, 32.32881504, ..., 32.32881504,\n        32.32807786, 32.32770907],\n       ...,\n       [49.99324171, 50.21420708, 50.49226259, ..., 50.49226259,\n        50.21420708, 49.99324171],\n       [50.00501891, 50.22440337, 50.50234643, ..., 50.50234643,\n        50.22440337, 50.00501891],\n       [50.01094032, 50.22936197, 50.50726479, ..., 50.50726479,\n        50.22936197, 50.01094032]])</pre></li><li>lon(y, x)float6473.5 74.5 75.5 ... 72.96 72.99<pre>array([[73.49956612, 74.49869878, 75.49783265, ..., 70.50216735,\n        71.50130122, 72.50043388],\n       [73.49942579, 74.49827789, 75.49713157, ..., 70.50286843,\n        71.50172211, 72.50057421],\n       [73.49925634, 74.49776967, 75.496285  , ..., 70.503715  ,\n        71.50223033, 72.50074366],\n       ...,\n       [73.07927918, 73.19734462, 73.28416978, ..., 72.71583022,\n        72.80265538, 72.92072082],\n       [73.0423483 , 73.11531867, 73.16770504, ..., 72.83229496,\n        72.88468133, 72.9576517 ],\n       [73.01084559, 73.03703107, 73.05471492, ..., 72.94528508,\n        72.96296893, 72.98915441]])</pre></li><li>time_counter()float320.0<pre>array(0., dtype=float32)</pre></li><li>band()int641<pre>array(1)</pre></li><li>spatial_ref()int640<pre>array(0)</pre></li></ul></li><li>Data variables: (2)<ul><li>siconc_error(obs, y, x)float64dask.array&lt;chunksize=(1, 97, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   272.81 kiB   272.81 kiB   Shape   (1, 97, 360)   (1, 97, 360)   Dask graph   1 chunks in 29 graph layers   Data type   float64 numpy.ndarray  360 97 1 </li><li>siconc(y, x)float32dask.array&lt;chunksize=(97, 360), meta=np.ndarray&gt;  Array   Chunk   Bytes   136.41 kiB   136.41 kiB   Shape   (97, 360)   (97, 360)   Dask graph   1 chunks in 12 graph layers   Data type   float32 numpy.ndarray  360 97 </li></ul></li><li>Indexes: (1)<ul><li>obsPandasIndex<pre>PandasIndex(Index(['NSIDC'], dtype='object', name='obs'))</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>Finally, we can also compute the aggregate statistics quantifying the error between our ocean model &amp; the NSIDC SIC climatology by calling <code>.compute()</code> on the <code>.stats</code> attribute.</p> In\u00a0[7]: Copied! <pre>mv.stats.compute()\n</pre> mv.stats.compute() Out[7]: <pre>&lt;xarray.Dataset&gt; Size: 44B\nDimensions:                 ()\nCoordinates:\n    time_counter            float32 4B 0.0\n    band                    int64 8B 1\n    spatial_ref             int64 8B 0\nData variables:\n    Mean Absolute Error     float64 8B 0.03282\n    Mean Square Error       float64 8B 0.01041\n    Root Mean Square Error  float64 8B 0.102</pre>xarray.Dataset<ul><li>Dimensions:</li><li>Coordinates: (3)<ul><li>time_counter()float320.0<pre>array(0., dtype=float32)</pre></li><li>band()int641<pre>array(1)</pre></li><li>spatial_ref()int640<pre>array(0)</pre></li></ul></li><li>Data variables: (3)<ul><li>Mean Absolute Error()float640.03282<pre>array(0.03281638)</pre></li><li>Mean Square Error()float640.01041<pre>array(0.01040598)</pre></li><li>Root Mean Square Error()float640.102<pre>array(0.10200971)</pre></li></ul></li><li>Indexes: (0)<ul></ul></li><li>Attributes: (0)</li></ul> In\u00a0[8]: Copied! <pre>mv.compute_siconc_error(sic_name='siconc',\n                        obs_name='NSIDC',\n                        region='antarctic',\n                        time_bounds=slice('2001-01', '2020-12'),\n                        freq='sep',\n                        regrid_to='model',\n                        method='bilinear',\n                        stats=True,\n                        )\n</pre> mv.compute_siconc_error(sic_name='siconc',                         obs_name='NSIDC',                         region='antarctic',                         time_bounds=slice('2001-01', '2020-12'),                         freq='sep',                         regrid_to='model',                         method='bilinear',                         stats=True,                         ) <pre>/dssgfs01/working/otooth/Diagnostics/ValidOcean/ValidOcean/processing.py:208: RuntimeWarning: [longitude: -180.0, 180.0; latitude: -86.0, 90.0] bounds are outside the range of available observations data [longitude: -179.81810924750283, 179.81810924750283; latitude: -89.83681599961737, -39.36486911321109].\n  warnings.warn(warning_message, RuntimeWarning)\n/dssgfs01/working/otooth/Diagnostics/ValidOcean/ValidOcean/processing.py:208: RuntimeWarning: [longitude: -180.0, 180.0; latitude: -90.0, -39.0] bounds are outside the range of available model data [longitude: -179.99653278676575, 179.99031297181477; latitude: -85.78874492732504, 89.7417689202141].\n  warnings.warn(warning_message, RuntimeWarning)\n</pre> Out[8]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 281MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB -84.21 -84.21 -84.21 ... 50.23 50.01\n    lon            (y, x) float64 953kB 73.5 74.5 75.5 ... 72.95 72.96 72.99\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    siconc         (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:             (y: 129, x: 360)\nCoordinates:\n    band_nsidc          int64 8B 1\n    spatial_ref_nsidc   int64 8B 0\n    lat_nsidc           (y, x) float64 372kB -84.21 -84.21 ... -39.45 -39.45\n    lon_nsidc           (y, x) float64 372kB 73.5 74.5 75.5 ... 70.5 71.5 72.5\n    time_counter_nsidc  float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    siconc_nsidc        (y, x) float64 372kB dask.array&lt;chunksize=(129, 360), meta=np.ndarray&gt;\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:       (obs: 1, y: 129, x: 360)\nCoordinates:\n  * obs           (obs) object 8B 'NSIDC'\n    lat           (y, x) float64 372kB -84.21 -84.21 -84.21 ... -39.45 -39.45\n    lon           (y, x) float64 372kB 73.5 74.5 75.5 76.5 ... 70.5 71.5 72.5\n    time_counter  float32 4B 0.0\n    band          int64 8B 1\n    spatial_ref   int64 8B 0\nDimensions without coordinates: y, x\nData variables:\n    siconc_error  (obs, y, x) float64 372kB dask.array&lt;chunksize=(1, 129, 360), meta=np.ndarray&gt;\n    siconc        (y, x) float32 186kB dask.array&lt;chunksize=(129, 360), meta=np.ndarray&gt;\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 44B\nDimensions:                 ()\nCoordinates:\n    time_counter            float32 4B 0.0\n    band                    int64 8B 1\n    spatial_ref             int64 8B 0\nData variables:\n    Mean Absolute Error     float64 8B dask.array&lt;chunksize=(), meta=np.ndarray&gt;\n    Mean Square Error       float64 8B dask.array&lt;chunksize=(), meta=np.ndarray&gt;\n    Root Mean Square Error  float64 8B dask.array&lt;chunksize=(), meta=np.ndarray&gt;</pre> <p>If we did want to visualise the results of our validation workflow, we could modify our previous example of the <code>plot_siconc_error</code> method as follows..</p> In\u00a0[9]: Copied! <pre>mv.plot_siconc_error(sic_name='siconc',\n                     obs_name='NSIDC',\n                     region='antarctic',\n                     time_bounds=slice('2001-01', '2020-12'),\n                     figsize=(15,12),\n                     freq='sep',\n                     regrid_to='model',\n                     method='bilinear',\n                     source_plots=True,\n                     stats=True,\n                     )\n</pre> mv.plot_siconc_error(sic_name='siconc',                      obs_name='NSIDC',                      region='antarctic',                      time_bounds=slice('2001-01', '2020-12'),                      figsize=(15,12),                      freq='sep',                      regrid_to='model',                      method='bilinear',                      source_plots=True,                      stats=True,                      ) <pre>/dssgfs01/working/otooth/Diagnostics/ValidOcean/ValidOcean/processing.py:208: RuntimeWarning: [longitude: -180.0, 180.0; latitude: -86.0, 90.0] bounds are outside the range of available observations data [longitude: -179.81810924750283, 179.81810924750283; latitude: -89.83681599961737, -39.36486911321109].\n  warnings.warn(warning_message, RuntimeWarning)\n/dssgfs01/working/otooth/Diagnostics/ValidOcean/ValidOcean/processing.py:208: RuntimeWarning: [longitude: -180.0, 180.0; latitude: -90.0, -39.0] bounds are outside the range of available model data [longitude: -179.99653278676575, 179.99031297181477; latitude: -85.78874492732504, 89.7417689202141].\n  warnings.warn(warning_message, RuntimeWarning)\n</pre> Out[9]: <pre>array([&lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;, &lt;GeoAxes: &gt;], dtype=object)</pre> In\u00a0[10]: Copied! <pre># Define model grid cell area:\nds_siconc['areacello'] = ds_domain['e1t'].squeeze() * ds_domain['e2t'].squeeze()\n\n# Creating a new ModelValidator object:\nmv = ModelValidator(mdl_data=ds_siconc)\nmv\n</pre> # Define model grid cell area: ds_siconc['areacello'] = ds_domain['e1t'].squeeze() * ds_domain['e2t'].squeeze()  # Creating a new ModelValidator object: mv = ModelValidator(mdl_data=ds_siconc) mv Out[10]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 282MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB -84.21 -84.21 -84.21 ... 50.23 50.01\n    lon            (y, x) float64 953kB 73.5 74.5 75.5 ... 72.95 72.96 72.99\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    siconc         (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    areacello      (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*</pre> In\u00a0[11]: Copied! <pre>mv.compute_siarea_timeseries(sic_name='siconc',\n                             area_name='areacello',\n                             obs_name='NSIDC',\n                             region='arctic',\n                             time_bounds=slice('2001-01', '2020-12'),\n                             stats=True,\n                             )\n</pre> mv.compute_siarea_timeseries(sic_name='siconc',                              area_name='areacello',                              obs_name='NSIDC',                              region='arctic',                              time_bounds=slice('2001-01', '2020-12'),                              stats=True,                              ) Out[11]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 282MB\nDimensions:        (y: 331, x: 360, time: 585)\nCoordinates:\n    lat            (y, x) float64 953kB -84.21 -84.21 -84.21 ... 50.23 50.01\n    lon            (y, x) float64 953kB 73.5 74.5 75.5 ... 72.95 72.96 72.99\n    time_centered  (time) datetime64[ns] 5kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 5kB 1976-01-16T12:00:00 ... 2024-09-16\n    time_counter   float32 4B 0.0\nDimensions without coordinates: y, x\nData variables:\n    siconc         (time, y, x) float32 279MB dask.array&lt;chunksize=(1, 331, 360), meta=np.ndarray&gt;\n    mask           (y, x) int8 119kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n    areacello      (y, x) float64 953kB dask.array&lt;chunksize=(331, 360), meta=np.ndarray&gt;\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 4kB\nDimensions:            (time_nsidc: 240)\nCoordinates:\n    band_nsidc         int64 8B ...\n    spatial_ref_nsidc  int64 8B ...\n  * time_nsidc         (time_nsidc) datetime64[ns] 2kB 2001-01-15 ... 2020-12-15\nData variables:\n    siarea_nsidc       (time_nsidc) float64 2kB dask.array&lt;chunksize=(12,), meta=np.ndarray&gt;\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 6kB\nDimensions:        (obs: 1, time: 240)\nCoordinates:\n  * obs            (obs) object 8B 'NSIDC'\n    time_counter   float32 4B 0.0\n    time_centered  (time) datetime64[ns] 2kB dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n  * time           (time) datetime64[ns] 2kB 2001-01-16T12:00:00 ... 2020-12-...\nData variables:\n    siarea         (obs, time) float64 2kB dask.array&lt;chunksize=(1, 1), meta=np.ndarray&gt;\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 60B\nDimensions:                          (obs: 1)\nCoordinates:\n  * obs                              (obs) object 8B 'NSIDC'\n    time_counter                     float32 4B 0.0\n    band                             int64 8B ...\n    spatial_ref                      int64 8B ...\nData variables:\n    Mean Absolute Error              float64 8B dask.array&lt;chunksize=(), meta=np.ndarray&gt;\n    Mean Square Error                float64 8B dask.array&lt;chunksize=(), meta=np.ndarray&gt;\n    Root Mean Square Error           float64 8B dask.array&lt;chunksize=(), meta=np.ndarray&gt;\n    Pearson Correlation Coefficient  (obs) float64 8B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;</pre> In\u00a0[12]: Copied! <pre>mv.plot_siarea_timeseries(sic_name='siconc',\n                          area_name='areacello',\n                          obs_name='NSIDC',\n                          region='arctic',\n                          time_bounds=slice('2001-01', '2020-12'),\n                          figsize=(8,4),\n                          plot_kwargs={'linewidth':3},\n                          stats=True,\n                          )\n</pre> mv.plot_siarea_timeseries(sic_name='siconc',                           area_name='areacello',                           obs_name='NSIDC',                           region='arctic',                           time_bounds=slice('2001-01', '2020-12'),                           figsize=(8,4),                           plot_kwargs={'linewidth':3},                           stats=True,                           ) Out[12]: <pre>&lt;Axes: xlabel='Time', ylabel='Sea Ice Area (km$^2$)'&gt;</pre>"},{"location":"ex3_sea_ice_validation_workflow/#sea-ice-validation-workflows-with-validocean","title":"Sea Ice Validation Workflows with ValidOcean\u00b6","text":""},{"location":"ex3_sea_ice_validation_workflow/#description","title":"Description:\u00b6","text":"<p>This tutorial notebook explores further validation workflows available in ValidOcean using the ModelValidator class.</p> <p>To demonstrate a typical sea ice validation workflow, we will use outputs from the National Oceanography Centre's Near-Present-Day global 1-degree (eORCA1-NPD-ERA5v1) ocean sea-ice simulation introduced in Example 1, which represents the historical period from 1976 - present.</p> <p>For more details on this model configuration and the available outputs, users should explore the Near-Present-Day documentation here.</p>"},{"location":"ex3_sea_ice_validation_workflow/#contact","title":"Contact:\u00b6","text":"<p>Ollie Tooth (oliver.tooth@noc.ac.uk)</p>"},{"location":"ex3_sea_ice_validation_workflow/#accessing-preparing-example-ocean-model-data","title":"Accessing &amp; Preparing Example Ocean Model Data\u00b6","text":"<p>We will begin by using the xarray Python library to load an example dataset from the eORCA1-NPD-ERA5v1 outputs available on the JASMIN Object Store.</p> <p>As in Examples 1 &amp; 2, we will load the NEMO ocean model domain variables first. Then, we will load monthly mean sea ice concentration data.</p> <p>Before creating a <code>ModelValidator()</code> object, we need to update our ocean model coordinate variables to conform to the standard names used by ValidOcean &amp; add a land/ocean mask file for regridding.</p>"},{"location":"ex3_sea_ice_validation_workflow/#creating-a-modelvalidator","title":"Creating A ModelValidator\u00b6","text":"<p>Now let's create a new <code>ModelValidator()</code> object using our monthly sea surface temperature data.</p>"},{"location":"ex3_sea_ice_validation_workflow/#plotting-model-observations-sea-ice-concentration-bias","title":"Plotting (Model - Observations) Sea Ice Concentration Bias\u00b6","text":"<p>Consider the typical scenario where we have just completed a ocean model simulation and would now like to know how closely the Arctic sea ice state in our hindcast reflects the real ocean. A standard validation workflow would be to calculate the difference or bias between the March (maximum) or September (minimum) climatologies of sea ice concentration (SIC) simulated in the model and recorded in observations.</p> <p>As in Example 2, we can divide this validation workflow into several steps:</p> <ol> <li>Select &amp; load our sea ice observation dataset</li> </ol> <ul> <li>We will use the 2001-2020 March Arctic climatology of the National Snow &amp; Ice Data Centre (NSIDC) Sea Ice Index version 3 product.</li> </ul> <ol> <li>Load &amp; subset our ocean model dataset</li> </ol> <ul> <li>Both a spatial &amp; temporal subsetting is needed to match the observations since NSIDC has only regional coverage.</li> </ul> <ol> <li><p>Calculate the SIC climatology from our ocean model dataset.</p> </li> <li><p>Regrid either the model climatology onto the observations grid or vice versa</p> </li> </ol> <ul> <li>We regrid the observations climatology onto the model grid using bilinar interpolation.</li> </ul> <ol> <li>Calculate the (model - observation) error &amp; aggregated statistics</li> </ol> <ul> <li>We return the MAE, MSE and RMSE.</li> </ul> <ol> <li>Plot the (model - observation) SIC error</li> </ol> <ul> <li>We also include the model &amp; observation SIC climatologies.</li> </ul> <p>As highlighted in Example 2, the true value of ValidOcean is that all of the steps are perfomed for you by using the plot_sic_error() method of the ModelValidator...</p>"},{"location":"ex3_sea_ice_validation_workflow/#results-of-our-validation-workflow","title":"Results of our Validation Workflow\u00b6","text":"<p>In addition to the plot shown above, calling the <code>plot_sic_error()</code> method has populated the <code>.obs</code>, <code>.results</code> and <code>.stats</code> attributes with the results of our validation workflow.</p> <p>Let's start by exploring the observational data which has been added to the <code>.obs</code> attribute.</p> <p>Here, we can see that a new DataArray <code>siconc_nsidc</code> has been added to the observations dataset as a result of regridding the NSIDC (2001-2020) climatology onto our original model grid using bilinear interpolation (shown in panel (b) above).</p>"},{"location":"ex3_sea_ice_validation_workflow/#validation-without-visualisation","title":"Validation Without Visualisation\u00b6","text":"<p>To perform an Antarctic SIC validation workflow without visualising the results, we can alternatively use the <code>.compute_sic_error()</code> method, which returns all of the results discussed above without the matplotlib axes object.</p> <p>To demonstrate this method, we will create a new <code>ModelValidator</code> object using our ocean model dataset and perform a September (minimum) SIC validation workflow as follows...</p>"},{"location":"ex3_sea_ice_validation_workflow/#validating-aggregated-diagnostics","title":"Validating Aggregated Diagnostics\u00b6","text":"<p>So far, we have seen how to calculate and visualise biases between 2-dimensional ocean model outputs and observations. However, we often would like to validate an aggregated diagnostic calculated from a multi-dimensional field; for example, total sea ice area calculated from the sea ice concentration field.</p> <p>To validate an aggregated diagnostic, we can define a workflow in several stages:</p> <ol> <li>Select &amp; load our sea ice observation dataset</li> </ol> <ul> <li>We will use the Arctic sea ice area diagnostic calculated from the National Snow &amp; Ice Data Centre (NSIDC) Sea Ice Index version 3 product (2001-2020).</li> </ul> <ol> <li>Load &amp; subset our ocean model dataset</li> </ol> <ul> <li>Both a spatial &amp; temporal subsetting is needed to match the observations since NSIDC has only regional coverage.</li> </ul> <ol> <li><p>Calculate the sea ice area from our ocean model dataset.</p> </li> <li><p>Calculate aggregated statistics</p> </li> </ol> <ul> <li>We return the MAE, MSE and RMSE.</li> </ul> <ol> <li>Plot the sea ice area time series</li> </ol> <p>As highlighted above, the true value of ValidOcean is that all of the steps are perfomed for you by using the plot_sic_error() method of the ModelValidator...</p>"},{"location":"ex3_sea_ice_validation_workflow/#next-steps","title":"Next Steps...\u00b6","text":"<p>In this tutorial, we have seen how to perform two typical ocean model validation worfklows for sea ice data using the ModelValidator object.</p> <p>Next, we will explore how to use the ModelValidator to load ocean observations datasets in the ex4_ocean_observations.ipynb notebook.</p>"},{"location":"ex4_ocean_observations/","title":"Accessing Ocean Observations with ValidOcean","text":"In\u00a0[1]: Copied! <pre># -- Import required libraries -- #\nimport xarray as xr\nfrom ValidOcean import ModelValidator\nfrom ValidOcean import DataLoader\n</pre> # -- Import required libraries -- # import xarray as xr from ValidOcean import ModelValidator from ValidOcean import DataLoader <pre>/home/otooth/miniconda3/envs/env_nemo_validation/lib/python3.12/site-packages/esmpy/interface/loadESMF.py:94: VersionWarning: ESMF installation version 8.8.0, ESMPy version 8.8.0b0\n  warnings.warn(\"ESMF installation version {}, ESMPy version {}\".format(\n</pre> In\u00a0[2]: Copied! <pre># Create an empty ModelValidator object:\nmv = ModelValidator()\n\n# Load NSIDC Arctic Sea Ice Concentration data:\nmv.load_observations(obs_name='NSIDC',\n                     var_name='siconc',\n                     region='arctic',\n                     )\n</pre> # Create an empty ModelValidator object: mv = ModelValidator()  # Load NSIDC Arctic Sea Ice Concentration data: mv.load_observations(obs_name='NSIDC',                      var_name='siconc',                      region='arctic',                      ) <pre>/dssgfs01/working/otooth/Diagnostics/ValidOcean/ValidOcean/processing.py:208: RuntimeWarning: [longitude: -180, 180; latitude: -90, 90] bounds are outside the range of available observations data [longitude: -180.0, 179.81397539549246; latitude: 31.102671752463447, 89.83681599961737].\n  warnings.warn(warning_message, RuntimeWarning)\n</pre> Out[2]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 605MB\nDimensions:            (y_nsidc: 448, x_nsidc: 304, time_nsidc: 553)\nCoordinates:\n    band_nsidc         int64 8B 1\n    lat_nsidc          (y_nsidc, x_nsidc) float64 1MB 31.1 31.2 ... 34.58 34.47\n    lon_nsidc          (y_nsidc, x_nsidc) float64 1MB 168.3 168.1 ... -9.999\n    spatial_ref_nsidc  int64 8B 0\n  * time_nsidc         (time_nsidc) datetime64[ns] 4kB 1978-11-15 ... 2025-01-15\n  * x_nsidc            (x_nsidc) float64 2kB -3.838e+06 -3.812e+06 ... 3.738e+06\n  * y_nsidc            (y_nsidc) float64 4kB 5.838e+06 5.812e+06 ... -5.338e+06\nData variables:\n    siconc_nsidc       (time_nsidc, y_nsidc, x_nsidc) float64 603MB dask.array&lt;chunksize=(12, 448, 304), meta=np.ndarray&gt;\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*</pre> <p>In the example above, we created an empty <code>ModelValidator()</code> object and then used the <code>.load_observations()</code> method to load a lazy Dataset (i.e., comprised of dask arrays which are not yet loaded into memory) including Arctic sea ice concentration data for the entire period of available observations (1978-2025).</p> <p>Behind the scenes, the <code>.load_observations()</code> method calls the private <code>._load_obs_data()</code> method, which in-turn creates an instance of the <code>NSIDCLoader()</code> class and calls its <code>._load_data()</code> method. This is the standard process for loading ocean observations data in ValidOcean, including by the plotting and compute methods introduced in Examples 2 &amp; 3. The resulting ocean observations dataset is accessible via the <code>.obs</code> attribute.</p> <p>Let's consider another example where we would like to load monthly sea surface temperature observations from the Met Office Hadley Centre Monthly Median (HadISST1) product for a regional subdomain and a custom time window...</p> In\u00a0[3]: Copied! <pre># Create a new, empty ModelValidator object:\nmv = ModelValidator()\n\n# Load NOAA OISSTv2 SST data:\nmv.load_observations(obs_name='HadISST',\n                     var_name='sst',\n                     lon_bounds=(-75, 5),\n                     lat_bounds=(20, 90),\n                     time_bounds=slice('2020-01', '2025-12'),\n                     freq=None,\n                     )\n</pre> # Create a new, empty ModelValidator object: mv = ModelValidator()  # Load NOAA OISSTv2 SST data: mv.load_observations(obs_name='HadISST',                      var_name='sst',                      lon_bounds=(-75, 5),                      lat_bounds=(20, 90),                      time_bounds=slice('2020-01', '2025-12'),                      freq=None,                      ) <pre>/dssgfs01/working/otooth/Diagnostics/ValidOcean/ValidOcean/processing.py:131: RuntimeWarning: time_bounds 2020-01 - 2025-12 are outside the range of available observations data 1870-01-16 - 2024-09-16.\n  warnings.warn(warning_message, RuntimeWarning)\n</pre> Out[3]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:       (lat_hadisst: 70, lon_hadisst: 80, time_hadisst: 57)\nCoordinates:\n  * lat_hadisst   (lat_hadisst) float32 280B 89.5 88.5 87.5 ... 22.5 21.5 20.5\n  * lon_hadisst   (lon_hadisst) float32 320B -74.5 -73.5 -72.5 ... 2.5 3.5 4.5\n  * time_hadisst  (time_hadisst) datetime64[ns] 456B 2020-01-16T12:00:00 ... ...\nData variables:\n    sst_hadisst   (time_hadisst, lat_hadisst, lon_hadisst) float32 1MB dask.array&lt;chunksize=(57, 70, 80), meta=np.ndarray&gt;\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*</pre> In\u00a0[4]: Copied! <pre># Plotting the time-mean SST:\nmv.obs['sst_hadisst'].mean(dim='time_hadisst').plot()\n</pre> # Plotting the time-mean SST: mv.obs['sst_hadisst'].mean(dim='time_hadisst').plot() Out[4]: <pre>&lt;matplotlib.collections.QuadMesh at 0x14740f37b980&gt;</pre> In\u00a0[5]: Copied! <pre>class MyLoader(DataLoader):\n    \"\"\"\n    Example DataLoader to load ocean observations stored\n    on a local filesystem or cloud object storage.\n\n    Parameters\n    ----------\n    var_name : str, default: ``my_variable``\n        Name of variable to load from example ocean observations.\n        Options include ``my_variable``, ``my_other_variable``.\n    time_bounds : slice, str, default: None\n        Time bounds to compute climatology using example ocean observations.\n        Default is ``None``, meaning the entire dataset is considered.\n    lon_bounds : tuple, default: None\n        Longitude bounds to extract from example ocean observations.\n        Default is ``None``, meaning the entire longitude range is loaded.\n    lat_bounds : tuple, default: None\n        Latitude bounds to extract from example ocean observations.\n        Default is ``None``, meaning the entire latitude range is loaded.\n    freq : str, default: ``None``\n        Climatology frequency of the example ocean observations.\n        Options include ``None``, ``total``, ``seasonal``, ``monthly``,\n        ``jan``, ``feb`` etc. for individual months. Default is``None``\n        meaning no climatology is computed from monthly data.\n    \"\"\"\n    def __init__(self,\n                 var_name: str = 'my_variable',\n                 region: str | None = None,\n                 time_bounds: slice | str | None = None,\n                 lon_bounds: tuple | None = None,\n                 lat_bounds: tuple | None = None,\n                 freq: str | None = None,\n                 ):\n\n        # -- Verify Inputs -- #\n        if var_name not in ['my_variable', 'my_other_variable']:\n            raise ValueError(\"``var_name`` must be one of 'my_variable', 'my_other_variable'.\")\n\n        # -- Initialise DataLoader -- #\n        super().__init__(var_name=var_name,\n                         region=region,\n                         source='jasmin-os', # &lt;- Modify to your cloud object storage or local filepath.\n                         time_bounds=time_bounds,\n                         lon_bounds=lon_bounds,\n                         lat_bounds=lat_bounds,\n                         freq=freq)\n\n    def _load_data(self) -&gt; xr.DataArray:\n        \"\"\"\n        Load example ocean observations data &amp; optionally\n        compute climatology using cloud object storage or\n        a local filesystem.\n\n        Returns\n        -------\n        xarray.DataArray\n            Dataset storing example ocean observations data\n            or option climatology at specified frequency.\n        \"\"\"\n        # Load data from our chosen source:\n        url = f\"{self._source}/Example/Example_global_monthly_1980_2025/\"\n\n        # Data to inherit source attributes:\n        source = xr.open_zarr(url, consolidated=True) # &lt;- Alternatively, use xr.open_dataset() for NetCDF files.\n        data = source[self._var_name]\n        data.attrs = source.attrs\n\n        # Extract observations for specified time, longitude and latitude bounds:\n        data = _apply_spatial_bounds(data, lon_bounds=self._lon_bounds, lat_bounds=self._lat_bounds)\n\n        if isinstance(self._time_bounds, slice):\n            data = _apply_time_bounds(data, time_bounds=self._time_bounds)\n\n        # Compute climatology:\n        if self._freq is not None:\n            data = _compute_climatology(data, freq=self._freq)\n\n        # Add spatial bounds to attributes:\n        data.attrs[\"lon_bounds\"], data.attrs[\"lat_bounds\"] = _get_spatial_bounds(lon=data[\"lon\"], lat=data[\"lat\"])\n\n        return data\n</pre> class MyLoader(DataLoader):     \"\"\"     Example DataLoader to load ocean observations stored     on a local filesystem or cloud object storage.      Parameters     ----------     var_name : str, default: ``my_variable``         Name of variable to load from example ocean observations.         Options include ``my_variable``, ``my_other_variable``.     time_bounds : slice, str, default: None         Time bounds to compute climatology using example ocean observations.         Default is ``None``, meaning the entire dataset is considered.     lon_bounds : tuple, default: None         Longitude bounds to extract from example ocean observations.         Default is ``None``, meaning the entire longitude range is loaded.     lat_bounds : tuple, default: None         Latitude bounds to extract from example ocean observations.         Default is ``None``, meaning the entire latitude range is loaded.     freq : str, default: ``None``         Climatology frequency of the example ocean observations.         Options include ``None``, ``total``, ``seasonal``, ``monthly``,         ``jan``, ``feb`` etc. for individual months. Default is``None``         meaning no climatology is computed from monthly data.     \"\"\"     def __init__(self,                  var_name: str = 'my_variable',                  region: str | None = None,                  time_bounds: slice | str | None = None,                  lon_bounds: tuple | None = None,                  lat_bounds: tuple | None = None,                  freq: str | None = None,                  ):          # -- Verify Inputs -- #         if var_name not in ['my_variable', 'my_other_variable']:             raise ValueError(\"``var_name`` must be one of 'my_variable', 'my_other_variable'.\")          # -- Initialise DataLoader -- #         super().__init__(var_name=var_name,                          region=region,                          source='jasmin-os', # &lt;- Modify to your cloud object storage or local filepath.                          time_bounds=time_bounds,                          lon_bounds=lon_bounds,                          lat_bounds=lat_bounds,                          freq=freq)      def _load_data(self) -&gt; xr.DataArray:         \"\"\"         Load example ocean observations data &amp; optionally         compute climatology using cloud object storage or         a local filesystem.          Returns         -------         xarray.DataArray             Dataset storing example ocean observations data             or option climatology at specified frequency.         \"\"\"         # Load data from our chosen source:         url = f\"{self._source}/Example/Example_global_monthly_1980_2025/\"          # Data to inherit source attributes:         source = xr.open_zarr(url, consolidated=True) # &lt;- Alternatively, use xr.open_dataset() for NetCDF files.         data = source[self._var_name]         data.attrs = source.attrs          # Extract observations for specified time, longitude and latitude bounds:         data = _apply_spatial_bounds(data, lon_bounds=self._lon_bounds, lat_bounds=self._lat_bounds)          if isinstance(self._time_bounds, slice):             data = _apply_time_bounds(data, time_bounds=self._time_bounds)          # Compute climatology:         if self._freq is not None:             data = _compute_climatology(data, freq=self._freq)          # Add spatial bounds to attributes:         data.attrs[\"lon_bounds\"], data.attrs[\"lat_bounds\"] = _get_spatial_bounds(lon=data[\"lon\"], lat=data[\"lat\"])          return data  In\u00a0[6]: Copied! <pre>ModelValidator(mdl_data=None, dataloader=MyLoader())\n</pre> ModelValidator(mdl_data=None, dataloader=MyLoader()) Out[6]: <pre>\n&lt;ModelValidator&gt;\n\n-- Model Data --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Observations --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Results --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*\n\n-- Stats --\n\n&lt;xarray.Dataset&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*</pre>"},{"location":"ex4_ocean_observations/#accessing-ocean-observations-with-validocean","title":"Accessing Ocean Observations with ValidOcean\u00b6","text":""},{"location":"ex4_ocean_observations/#description","title":"Description:\u00b6","text":"<p>This tutorial notebook explores how to use the ModelValidator class to access ocean observations stored in the cloud.</p> <p>To demonstrate how to access ocean observations, we will first introduce the core concept of DataLoaders, which are responsible for loading data from Analysis-Ready Cloud Optimised (ARCO) zarr stores in the JASMIN Object Store.</p>"},{"location":"ex4_ocean_observations/#contact","title":"Contact:\u00b6","text":"<p>Ollie Tooth (oliver.tooth@noc.ac.uk)</p>"},{"location":"ex4_ocean_observations/#introducing-dataloaders","title":"Introducing DataLoaders\u00b6","text":"<p>One of the foundational concepts behind the ValidOcean library was to enable users to access ocean observations data stored in zarr format in the cloud through a method that is independent of their local or remote machine.</p> <p>A major benefit of storing ocean observations in cloud object storage is that users can then directly access this data via a read-only URL.</p> <p>In ValidOcean, we've taken this data accessibility one step further by creating DataLoaders which load ocean observations from the JASMIN object store and pre-process them into a standardised xarray Dataset.</p> <p>To understand how DataLoaders work, let's start with an example of accessing the National Snow &amp; Ice Data Centre (NSIDC) Sea Ice Index version 3...</p>"},{"location":"ex4_ocean_observations/#visualising-ocean-observations","title":"Visualising Ocean Observations\u00b6","text":"<p>Let's make a quick visualisation of the time-mean regional OISSTv2 sea surface temperatures by accessing the data directly via the <code>.obs</code> attribute...</p>"},{"location":"ex4_ocean_observations/#custom-dataloaders","title":"Custom DataLoaders\u00b6","text":"<p>A major advantage of the generalised approach to loading observations introduced above is that users can easily extend the ValidOcean library by adding custom DataLoaders.</p> <p>Below we can see an example DataLoader which inherits the DataLoader Abstract Base Class &amp; could be used to load ocean observations from a local file system or alternative cloud object storage provider...</p>"},{"location":"ex4_ocean_observations/#anatomy-of-a-custom-dataloader","title":"Anatomy of a Custom DataLoader\u00b6","text":"<p>Initially, there looks like a lot of code in the example DataLoader shown above, but actually there are only two components needed to create a DataLoader.</p> <ol> <li>Defining our Class with an <code>__init__()</code> method.</li> </ol> <ul> <li>This is how ValidOcean will create an instance of the class. Notice that we use <code>super().__init__()</code> to pass the input arguments to our <code>MyLoader()</code> class to the <code>DataLoader()</code> abstract base class that it inherits. This step ensures that our new DataLoader conforms to the standards required for use in ValidOcean.</li> </ul> <ol> <li>Defining a <code>._load_data()</code> method.</li> </ol> <ul> <li>This method is a requirement for every DataLoader and should use the attributes of the DataLoader to load ocean observations as an xarray Dataset. In the example above, we have included several useful pre-processing steps that you will find in each of the core DataLoaders included with ValidOcean.</li> </ul>"},{"location":"ex4_ocean_observations/#using-a-custom-dataloader","title":"Using a Custom DataLoader\u00b6","text":"<p>To use our custom DataLoader in a validation workflow, we can pass the <code>MyLoader()</code> object as the <code>dataloader</code> argument when creating a ModelValidator() object...</p>"},{"location":"ex4_ocean_observations/#next-steps","title":"Next Steps...\u00b6","text":"<p>In this tutorial, we have seen how to use the ModelValidator object to access ocean observations stored in the cloud &amp; how to add custom DataLoaders to extend the ValidOcean library.</p> <p>Next, start exploring the capabilities of ValidOcean for accelerating your own ocean model validation workflows!</p>"},{"location":"observations/","title":"Observations","text":"<p>Summary</p> <p>This page introduces the framework for accessing ocean observations via the ValidOcean package &amp; includes a catalog of currently available ocean observation datasets.</p>"},{"location":"observations/#dataloaders","title":"DataLoaders","text":"<p>One of the foundational concepts behind the ValidOcean library was to enable users to access ocean observations data stored in zarr format in the cloud through a method that is independent of their local or remote machine.</p> <p>A major benefit of storing ocean observations in cloud object storage is that users can then directly access this data via a read-only URL.</p> <p>In ValidOcean, we've taken this data accessibility one step further by creating DataLoaders which load ocean observations from the JASMIN object store and pre-process them into a standardised xarray Dataset.</p>"},{"location":"observations/#available-ocean-observations","title":"Available Ocean Observations","text":"<p>Section Currently Under Development: Come Back Soon!</p> <p>This section will include a catalog of DataLoaders currently available in the ValidOcean package to access ocean observations stored in cloud object storage.</p> <p>When using the ocean observations ARCO datasets available in ValidOcean, users should always acknowledge the original source of the data. Full details of the source, versions and pre-processing steps are available in the xarray <code>.attrs</code> property.</p> <ul> <li> <p>NOAA High-resolution Blended Analysis of Daily SST (OISSTv2) - sea surface temperature (sst) and sea ice concentration (siconc) dataset.</p> </li> <li> <p>NOAA National Snow &amp; Ice Data Center Sea Ice Index v3 (NSIDC) - sea ice concentration (siconc), sea ice extent (siext), sea ice area (siarea).</p> </li> <li> <p>Met Office Hadley Centre global sea ice &amp; sea surface temperature (HadISST1) - sea ice (siconc) and sea surface temperature (sst) data set - Academic Use Only.</p> </li> </ul>"},{"location":"user_guide/","title":"User Guide","text":"<p>Summary</p> <p>This page introduces the basic features of the ValidOcean package, including the ModelValidator object and the essential compute &amp; plot methods used to validate ocean model outputs.</p>"},{"location":"user_guide/#modelvalidator","title":"ModelValidator","text":""},{"location":"user_guide/#attributes","title":"Attributes","text":"<ul> <li> <p><code>data</code></p> </li> <li> <p><code>obs</code></p> </li> <li> <p><code>results</code></p> </li> <li> <p><code>stats</code></p> </li> </ul>"},{"location":"user_guide/#methods","title":"Methods","text":""},{"location":"user_guide/#compute","title":"Compute","text":"<ul> <li><code>compute_[var]_error()</code></li> </ul>"},{"location":"user_guide/#plot","title":"Plot","text":"<ul> <li><code>plot_[var]_error()</code></li> </ul>"}]}